\documentclass[12pt]{iopart}
% \usepackage[numbers,sort,compress]{natbib}
\usepackage{iopams}  
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan
    }
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{multirow}
\usepackage{color}
\usepackage{lineno}
\usepackage[normalem]{ulem}
\usepackage{placeins}
\usepackage{xspace}
\usepackage{soul}

\usepackage{geometry}
\geometry{left=2cm, right=2cm, bottom=1.5cm, top=1.5cm}
\linespread{1.5}

\renewcommand*\abstractname{Summary}
\newcommand{\NA}{\ensuremath{\text{---}}\xspace}
\newcommand{\PH}{\ensuremath{\mathrm{H}}\xspace}
\newcommand{\kt}{\ensuremath{k_{\mathrm{T}}}\xspace}
\newcommand{\ptmomentum}{\ensuremath{p_{\mathrm{T}}}\xspace}
\newcommand{\GeV}{\ensuremath{\,\text{Ge\hspace{-.08em}V}}\xspace}
\newcommand{\TeV}{\ensuremath{\,\text{Te\hspace{-.08em}V}}\xspace}
\newcommand{\hlsfml}{\texttt{hls4ml}\xspace}

\makeatletter
\def\@maketitle{%
  \newpage
  % original definition of \@maketitle minus the \newpage or \clearpage command
}


\begin{document}
\title[Real-time AI for HL-LHC and beyond (SNSF SG Proposal, T. K. \AA rrestad)]{
\underline{SNSF Starting Grant Proposal:}\\\vspace{5mm}
% {New Physics detection with unsupervised AI}}
{Detecting New Physics at 40 MHz:\\
Enhancing the CMS hardware event filtering system with real-time AI for High Luminosity LHC and beyond}}
\author{Thea Kl\ae boe \AA rrestad}
\address{ETH Z端rich, Switzerland}
 \ead{thea.aarrestad@cern.ch}
\date{\today}



\begin{abstract}
Despite being our current best description of nature, we know that The Standard Model of particle physics breaks down at short distances. It is an effective field theory, to be replaced by something more fundamental at shorter distance scales. Nevertheless, searches for new physics beyond the standard model at the CERN Large Hadron Collider (LHC) have thus far been futile. The reasons for this may be that a) the cross section of the signal is too small and more data is needed, or b) that we are not sensitive to it using our currents methods of detection. This project will address both of these using state-of-the art Machine Learning. The LHC will undergo substantial upgrades to its High-Luminosity phase (HL-LHC), with construction work beginning in 2026 and first collisions taking place in 2029~\cite{LHC2022LongTerm}. This upgrade will result in a factor of ten times more data delivered to the CMS experiment, allowing for the measurement of extremely rare processes. Nevertheless, this increase in accumulated data comes at the cost of data complexity: the number of concurrent proton-proton collisions will increase from the present 60 to as many as 200 in the HL-LHC phase.

The ultimate limiting factor for many searches for New Physics with the CMS experiment is the Level-1 hardware trigger. Here, it is decided which of the 40 million proton-proton collision events occurring every second are to be stored for further processing, and which are to be discarded and hence are lost forever. Over 98 \% of all collision events are rejected at this stage so high selection accuracy is crucial for physics analyses. Due to the extreme increase in data complexity, the Level-1 trigger will be completely redesigned for HL-LHC in order to cope with a data rate of 63 Tb/s, corresponding to 5\% of the total internet traffic, within a latency of 12~$\mu$s~\cite{tdr}. This is ten orders of magnitude more data than the current LHC Level-1 trigger receives, with only a factor of 3 more time to process the data. Much of the data comes from the inclusion of information from the CMS tracker, offering a highly precise measurement of charged particle momentum, as well as more granular detectors. The inclusion of this information will result in an unprecedented resolution at Level-1; after 12~$\mu$s we will have the same level of detail of information of every single event as we would only have offline only for a small subset of the data. 
The growing complexity and volume of data necessitate algorithms capable of swiftly and accurately processing high-dimensional information. This requirement has driven substantial research into machine learning-based algorithms, which have achieved a high level of success~\cite{tdr}.
This work will become increasingly important in the years leading up to the start of data taking at the HL-LHC and will also be of significant importance for future experiments at the new Future Circular Collider, or other scientific experiments.
Besides ensuring that data resolution and physics quality are upheld to enable analyses with the same level of sensitivity as the current LHC, the HL-LHC era will be a time dedicated to exhaustive exploration in the search for New Physics, leaving no stone unturned.
Despite conducting hundreds of searches for a wide variety of potential new particles, none have been discovered to date. However, there remain unexplored areas within the data where a signal might be detected. Addressing this will necessitate a transformation in our approach to conducting these searches.

In this proposal, I suggest two complementary projects aimed at addressing points a) and b) and the challenges listed above. The first project involves fundamentally changing the process of event reconstruction and selection in the Level-1 trigger. Taking advantage of techniques from natural language processing, leveraging models trained self-supervised on vast amounts of data, we will move towards end-to-end ML-based reconstruction for hardware trigger systems. Operating on low-level information, and bypassing intermediate reconstruction steps, the aim is to simultaneously improve accuracy and processing time. Work developed here would apply to both the HL-LHC as well as future collider experiments. The approach uses ultra-fast ML algorithms running inference on field programmable gate arrays (FPGAs). This builds on the pioneering work currently being undertaken by my group at ETH. The reconstruction techniques developed for this project have the potential to be applied not only in particle physics, but also in data processing for large telescope arrays and neutrino experiments. The second project aims to develop a real-time, signal-independent discovery analysis. This analysis would have the capability to analyze every single collision within CMS, bypassing the Level-1 trigger by running in separate system. Utilizing open world learning, where a ML algorithm starts with a set of known classes and then detects and learns new ones over time from a non-stationary stream of data, the objective is to discover a wide array of potential new physics signatures in a manner not specific to any one signal.
This approach will help us maximize the utilization of the HL-LHC data collection period by ensuring that we do not overlook potential evidence of new physics simply because our search is concentrated in the wrong place.
\end{abstract}

\makeatother

\section{Section a: State-of-the-art and objectives}

At the CERN Large Hadron Collider (LHC), bunches of trillions of protons are brought to collide in the center of the four particle detectors around the LHC ring: ATLAS, CMS, ALICE and LHCb~\cite{LyndonEvans_2008}. This generates showers of new outgoing particles that produce signals in the particle detectors located around the interaction point. Due to the high frequency of collisions, every 25 ns, and large number of read-out channels, $\mathcal{O}(10)$~million, an enormous amount of data is generated. For the CMS experiment, this is $\mathcal{O}(100)$~Tb/s. These signals are read out using custom electronics mounted on support structures inside the detectors. This large data rate can not be read out and stored as it would require a significant amount of electronics and power supply inside the detector, obstructing the path of generated particles and reducing the detector sensitivity. Rather, the data rate is reduced by a two-stage filtering system, the trigger. While data is buffered inside the detector on detector front-end electronics, a subset of the detector information is sent through high speed optical links to a radiation-shielded cavern located next to the detector. Here, the first stage of the event filtering system, the Level-1 trigger (L1T), is responsible for reducing the data rate by two orders of magnitude within the buffering time of $\mathcal{O}(1)\mu s$. Due to the strict latency constraints, this processing is done fully in firmware on $\mathcal{O}(1000)$~field programmable gate arrays (FPGAs). The complexity of the data requires hundreds of reconstruction algorithms to run in parallel on several collision events simultaneously, which requires each algorithm to use minimal resources and time. The extreme latency and bandwidth requirements for the L1T system is unique and requires novel solutions in terms of IO and algorithms. Due to their ability to efficiently and accurately process and parameterize high dimensional input, Machine Learning (ML) algorithms are increasingly being explored as faster and better replacements of the classical algorithms currently in use in the L1T. The extremely low latency and resource requirements for ML algorithms deployed in the L1T system has led to the development of dedicated software libraries that facilitates the training and translation of ML models into efficient FPGA firmware; hls4ml~\cite{hls4ml,qkerashls4ml,Aarrestad_2021,Ghielmetti_2022} for deep neural networks and Conifer~\cite{Summers_2020} for decision trees, work I have been an integral part of for the past five years. This has allowed for the first deep neural networks to be integrated into the L1T system, work done during the first two years of my SNSF Ambizione grant, running inference within a few tens of nanoseconds~\cite{CMS-DP-2023-079} and expected to collect data for offline analysis in 2024.

\subsection{HL-LHC and beyond: Future Challenges}
With the rapid development of hls4ml and Conifer, deep neural network and decision tree sub-microsecond inference in L1T systems is a reality. However, there are significant challenges ahead. In order to study increasingly rare physics processes in the LHC detectors, the LHC will be upgraded to its High Luminosity phase (HL-LHC), allowing the experiments to collect a factor of ten more data~\cite{tdr}. On the other hand, this involves a significant trade-off: The number of simultaneous proton collisions will triple, leading to an increased event complexity. To cope with this, more granular detectors will be installed, demanding even more complex reconstruction algorithms due to the large increase in read-out channels. For the first time, the CMS L1T will also receive information from the inner tracking system. This will require the reconstruction of hundreds of charged particle tracks from thousands of detector hits, within a maximum latency of $5\mu s$. The input data rate to the L1T systems will increase by one order of magnitude, corresponding to around 5\% of the total internet traffic~\cite{tdr}. This necessitates the design and implementation of significantly more advanced ML algorithms for the reconstruction, correction and selection of proton collision events~\cite{Elabd2022GNNFPGAs,Brown2023NNPrimaryVertex,Iiyama2021DistanceWeightedGNN,Que2022OptimizingGNN,CMS2022NNAlgorithm,CMS2023ElectronReconstruction,CMS2023ContinualLearning}.

Having access to charged particle tracks and more granular information, will result in unprecedented resolution in the Level-1 system. This enhanced resolution is achieved by using these tracks in conjunction with data from the calorimeters and muon chambers. The process of assembling particles and delineating their paths through the detector is known as \textit{Particle Flow (PF)}~\cite{Sirunyan_2017}. For the first time, this method, which is the most powerful event reconstruction technique used in offline processing, will be implemented on FPGAs within the Level-1 trigger. The expected improvement from running these algorithms in the trigger is shown in Figure~\ref{fig:phase2_physics}. In order to select interesting events, one usually defines thresholds on certain physical quantities where every event which passes one or more thresholds is kept for further offline analysis. Lowering this threshold results in increased signal efficiency, for instance for potential New Physics processes, but will also lead to decreased signal purity, collecting more events deemed uninteresting.

Figure~\ref{fig:phase2_physics} shows the distribution of two key variables used in selecting events in the Level-1 trigger, for three different interesting physics processes. Events positioned to the right of the vertical lines are retained for further offline analysis, while the ones that fall on the left are discarded by the Level-1 trigger. The inclusion of tracks and PF (represented by the solid line) markedly enhances signal acceptance compared to the present default method (indicated by the dashed line), thereby improving the sensitivity in the search for these processes.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.39\textwidth]{figures/phase2_tauh.pdf}
  \includegraphics[width=0.39\textwidth]{figures/phase2_etmiss.pdf}
  \caption{Two physical properties used to select events in the Level-1 trigger: the transverse momentum of hadronically decaying $\tau$ particles (left) and the event missing transverse energy (right), for various interesting physics processes. Data points positioned to the left of the vertical lines are excluded by the trigger system. The integration of charged particle tracks and Particle Flow reconstruction into the Level-1 trigger enables a substantial reduction in selection thresholds, leading to improved signal efficiency.~\cite{tdr}.} % Caption
  \label{fig:phase2_physics} % Label for referencing
\end{figure}
In addition to enhancing signal acceptance, the availability of such high resolution for the first time opens up the possibility of conducting an analysis of data reconstructed at Level-1. This provides the unique opportunity to access information on every single collision occurring inside CMS, though at a slightly lower resolution than what is achievable offline. However, the challenge lies in the limited buffering time of $12\mu s$, rendering any attempt at such detailed analysis impractical.

Due to this, CMS will deploy a novel parasitic system that acquires the L1 trigger data at the full bunch crossing rate of 40 MHz and analyzes certain interesting topologies at the full rate~\cite{40MhzSc}, referred to as \textit{40 MHz scouting}. In this system one can either perform a real-time analysis of the L1 data and/or store a subset of the events in a compact format. This system will run in parallel to the Level-1 trigger and is the only place one will have access to the full CMS collision data, bypassing any trigger selection. This system will offer access to physics signatures that would evade the typical Level-1 to Offline chain, for instance signatures which have to large irreducible backgrounds (for instance narrow resonances of unknown mass) or cases where the signal identification itself requires an algorithm that cannot fit within the Level-1 fixed latency.
A sketch of this system is shown in Figure~\ref{fig:scouting}.
\begin{figure*}[htb!]
    \centering
    \includegraphics[width=0.69\textwidth]{figures/40mhz_scout.pdf}
    \caption{A sketch of the 40 MHz scouting system prototype planned for HL-LHC, which includes calorimeter- and muon-scouting. The system consists of a parallel readout chain which processes copies of the output streams feeding the Level-1 components. It consists of optical links connected to the L1 triggers, an input board consisting of FPGAs performing local pre-processing, zero-suppression and short-time buffering. This then feeds into other accelerators (GPUs, CPUs) that use distributed algorithms to extract features. Finally, the data goes into a distributed global stream processing system and is stored in a feature database~\cite{tdr}.
    }
    \label{fig:scouting}
\end{figure*}

\subsection{Model-independent searches for New Physics}
An important limiting factor in the search for New Physics is related to how such searches are designed. Typically, a specific New Physics model is assumed and a hypothesis test based on whether the assumed signal is present in the data or not is performed. Such a search is model-dependent and requires one to assume a given signal scenario before the search is conducted. However, there's a growing likelihood, evidenced by the absence of NP signals in years of data collection, that New Physics may manifest in ways not yet hypothesized. Recently, methods to look for Beyond Standard Model physics signatures in a model-agnostic way using machine learning have been proposed and explored, both at the trigger level~\cite{CMS-DP-2023-079,BELIS2024100091} and at the offline analysis level~\cite{Harris:2881089,BELIS2024100091}. My team at ETH Z端rich has made significant contributions to this area of work. However, developing methods to extract potential New Physics signatures in a statistically powerful and model-agnostic way, considering multiple variables of interest simultaneously, has proven itself challenging~\cite{D_Agnolo_2019}.

\subsection{Modern Machine Learning: Large language models and open world learning}
Concurrent with these advancements, significant progress in Modern Machine Learning is prompting a reevaluation of system design and analytical methods in particle physics. Large language models show excellent capabilities in learning tasks with minimal supervision when trained on large datasets~\cite{Radford2019LanguageMA,brown2020language}. The CMS detector generates petabytes of unlabeled data every second during data collection. This wealth of data holds the potential to be utilized for training robust ML-based models that accurately represent CMS data. Especially interesting are so called transformer models
This ability to learn without explicit need for labels, can also be harnessed to change the way searches for New Physics is performed. 
Additionally, an increasing number of algorithms are being trained on continuous and dynamically updating data streams. This is to accommodate the possibility of new, previously unseen classes suddenly emerging in the dataset.
In open-world learning~\cite{jafarzadeh2022review}, an agent starting with a set of known classes, learns to identify unknown entities, and learning these new elements over time from a dynamic data stream. In Ref.~\cite{cao2022openworld}, the authors explore such an approach to learning novel classes in a dataset by combining three loss functions: a supervised cross-entropy loss, a pairwise cross-entropy loss with adaptive uncertainty margin, and a regularization term.


\subsection{Objectives}
For the reasons above, there are two projects that are essential to undertake in preparation for the HL-LHC data collection and future experiments, both related to fundamentally changing the way ye collect and analyze data at the LHC . These projects form the foundation of this SNSF Starting grant proposal:

\begin{enumerate}
    \item \textbf{WP1 - Towards end-to-end smart triggering for HL-LHC and beyond:} The Level-1 trigger takes as input a large set of detector information, outputting a simple binary decision (keep or discard). This process is currently carried out through several reconstruction steps. My proposal, leveraging modern ML techniques and algorithms, is to bypass these intermediate stages and develop a direct end-to-end trigger reconstruction and selection algorithm. This could be executed either through a singular, large model or a consortium of specialized models. This approach allows us to enhance the efficiency of selection, improving both accuracy and computational resource requirements. This is crucial as detectors get more and more complex and data streams become difficult to manage. The model(s) must be capable of distributed inference across multiple FPGAs with microsecond latency. Besides improving data quality for the HL-LHC, this development is intended to lay the groundwork for future detectors and other experiments, such as large telescope arrays.
\item \textbf{WP2 - Real-Time, Model-Independent New Physics Detection through Open-World Learning:} The 40 MHz scouting system offers an unprecedented opportunity to look at the full collision data with no initial trigger bias. I propose to implement a real-time analysis utilizing open-world machine learning. This learning approach not only classifies data into known Standard Model (SM) categories but is also capable of identifying instances from unknown categories, such as potential New Physics processes.
\end{enumerate}

\noindent Each of these projects is substantial enough to be the subject of a PhD thesis. I propose allocating one student to each project, under the supervision of the PI and assisted by a Postdoctoral researcher. In the sections that follow, I will discuss the specifics of both projects.


\section{Section b: Methodology}


\section{WP1: Towards end-to-end smart triggering for HL-LHC and beyond}
\subsection{The Level-1 hardware trigger}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.69\textwidth]{figures/phase2_l1t.pdf}
    \caption{Diagram of the CMS L1 hardware trigger as foreseen for HL-LHC. The system is located in a radiation shielded cavern right next to the CMS detector and consists of hundreds of FPGAs mounted on custom boards. Each subsystem; calorimeters (orange), tracking detectors (green) and muon chambers (light blue), are first reconstructed locally on  hundreds of FPGAs. This information is then sent forward to a system responsible of correlating the information from all subdetectors using the Particle Flow algorithm (yellow). Finally, the global trigger receives all trigger information for the final decision (pink)~\cite{tdr}.}
    \label{fig:phase2}
\end{figure*}
The CMS Level-1 trigger is designed in a hierarchical way. Information from each subdetector is first processed and reconstructed locally. For instance for the calorimeter, the local reconstruction consists of clustering energy deposits into disentangled showers from individual particles. Local information from each sub system is passed on-wards and combined into particle-level information, using an algorithm called Particle Flow~\cite{Sirunyan_2017}. Specifically, Particle Flow combines measurements from the charged particle tracker with those from the electromagnetic and hadronic calorimeters. It outputs a collection of particles, detailing their energy, direction and type. These particles and their features are either used to construct other powerful variables, such as clustering them into a distinct spray of particles originating from the same parent particle, or they are directly employed as inputs for binary selection algorithms within the Global Trigger. For each local reconstruction of a subsystem, hundreds of FPGAs are used.

\subsection{Machine Learning-based reconstruction}
Recently, significant effort has been made into developing powerful and expressive ML algorithms for fast reconstruction. One example is the demonstration of a graph neural network for clustering of energy deposits in irregular geometry detectors~\cite{garnet}. A second example is a graph neural network based architecture that mimics Particle Flow; combining information from all sub detectors into particles of a given type. Such algorithms can be seen to perform as well, or better, in reconstructing the true features of a given event than their classical counterparts. This is shown in Figure~\ref{fig:mlpf}, where the solid green line corresponds to the true underlying value a feature, and the dashed lines to the classical Particle Flow (dashed blue) and Machine Learning based Particle Flow (dashed orange) reconstructed values.
\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.39\textwidth]{figures/mlpf_jetpt.pdf}
    \includegraphics[width=0.39\textwidth]{figures/mlpf_met.pdf}
    \caption{A comparison between the traditional Particle Flow reconstruction algorithm (dashed blue line) and a graph neural network-based particle-flow reconstruction (dashed orange line) is shown for two important trigger variables: the jet transverse momentum and the event's missing transverse energy. These are compared with the true values (solid green line)~\cite{mokhtar2023progress}.}
    \label{fig:mlpf}
\end{figure*}
In Ref.~\cite{mokhtar2023progress}, the authors quote a significant quantitative improvement using the ML based algorithm, with up to a factor of two improvement on some variables. Besides having the potential to improve the reconstruction accuracy, one of the main benefits of the Machine Learning based reconstruction is related to how this algorithm scales as a function of the input size. By utilizing smart graph-building techniques from the ML literature, dynamically learned graphs with a learned binning or transformers~\cite{vaswani2017attention} with fast attention~\cite{choromanski2022rethinking}, these algorithms can achieve favorable scaling. While the traditional Particle Flow algorithm shows a roughly quadratic scaling with the number of input particles, the inference time of the graph neural network-based model increases approximately linearly with the growing input size~\cite{Pata2023MLPF}. This is shown in Figure~\ref{fig:mlpf_comp}.
\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.79\textwidth]{figures/mlpf_compute.pdf}
    \caption{Inference time as a function of the number of particles per event for the traditional Particle Flow reconstruction algorithm (left) and a graph neural network-based particle-flow reconstruction (right). The latter is shown for different batch sizes $B$~\cite{Pata2023MLPF}.}
    \label{fig:mlpf_comp}
\end{figure*}

The ML based Particle Flow algorithms are currently being explored for offline reconstruction or for the High Level Trigger, but are not yet being developed for Level-1. This requires significant expertise in designing and deploying models in extremely high throughput and low latency environments, an area in which I have experience and aim to focus on in this project.

% Input for cluster-based MLPF ~ O(102) tracks or calorimeter clusters per event
% Input for hit-based MLPF ~ O(104) tracks or calorimeter hits per event

\subsection{AI engines and distributed inference: Accelerating reconstruction}
In order to deploy large models that must cope in extremely high throughput and low latency conditions, there is a need for dedicated systems and hardware. More and more dedicated AI processors are being developed to accelerate inference of large ML models. One example is the Xilinx Versal AI processors which contain 400 AI processors as well as FPGA fabric, where data can move back and forth between AI Engines and FPGA~\cite{XilinxVersalAI}. There is also significant research being done on distributed inference: where a single ML model can be split over multiple FPGAs and data can move quickly between the different FPGAs. This work is being pioneered by, among others, Gustavo Alonso's group at ETH~\cite{AlonsoWebpage}. This project will involve research into both deployment on current FPGA hardware, but also benchmarking of novel technologies.

\subsection{Putting it together: End-to-end}
Whereas ML is becoming crucial for improving existing algorithms in the Level-1 trigger, the ultimate goal is to replace all the intermediate steps of local reconstruction by a single powerful model or several large models trained in unison. This would allow for more efficient processing. Powerful models (e.g. transformer architectures) and techniques (like unsupervised pre-training on unlabeled data) from Natural Language Processing  that has the potential to go directly from detector hits to physics objects, approximating the full sequence of local reconstruction and combination, could be deployed on either 1) powerful AI engines or 2) using the existing interconnected FPGAs of the Level-1 trigger design to perform distributed inference. Finding novel, efficient and accurate ways of doing reconstruction during HL-LHC and for future experiments is not a choice, but a requirement. With the increase in input data size to Level-1 for HL-LHC, our current algorithms for doing reconstruction will be unsustainable.

\subsection{Project components}
In this project we will design and end-to-end ML based flow for hardware event reconstruction in the Level-1 trigger. This most important ingredients are:
\begin{enumerate}
    \item Starting from the work described above on offline ML reconstruction for Particle Flow, we will redesign this network for PF reconstruction in the Level-1 trigger. This includes redefining the algorithm to run on Level-1 inputs provided from the sub detectors, which will naturally have lower resolution than the inputs available offline. For this we would use synthetic data. This algorithm will then have to be translated into FPGA firmware, which additionally can run distributed over multiple FPGAs, such that it stays within the latency and resource budget of the trigger. For HL-LHC the hardware is fixed, powerful FPGAS interconnected via high speed optical links, but we would explore other accelerators as part of research into future detectors or for other scientific experiments. Here, we would rely on the expertise and infrastructure of Prof. G. Alonso's group and the Xilinx Compute Cluster at ETH Z端rich.
    \item Extensive validation of this model and testing under varying conditions using simulated data as well as data already collected by the CMS Experiment will be performed. It will be crucial to make sure the model is robust to changing external conditions in the CMS experiment, and for this we will explore a technique called continual learning or online learning~\cite{soutifcormerais2023comprehensive}. 
    \item After completion of the previous steps, we will attempt to include the local reconstruction of the sub detectors into the model, for true end-to-end processing. Here we would take the approach of what is referred to as a \textit{foundation model} in the ML literature~\cite{Bommasani2021FoundationModels}. A model, which can be similar to the ML Particle Flow model above, would be trained self-supervised on data collected by the CMS experiment or on synthetic data. The self-supervision signal could for instance be to mask a particle, or parts of a particle trajectory, from the event and teach the algorithm to learn to reconstruct it again. Or it could be to use a technique referred to as \textit{contrastive learning}~\cite{chen2020simple}, whereby data augmentation is utilized to train an alforithm to map any given input to its augmented self. Here, we would borrow and rely on work on foundation models in particle physics being pioneered in Ref.~\cite{foundation}
\end{enumerate}

An illustration of the final AI-powered end-to-end reconstruction design is showed in Figure~\ref{fig:e2e}.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/mlpf_foundation.pdf}
    \caption{The AI-powered end-to-end trigger reconstruction design.}
    \label{fig:e2e}
\end{figure*}

The techniques developed here generalizes to other scientific experiments as well, for instance the reconstruction of Cherenkov showers in the Cherenkov Telescope Array~\cite{CTAObservatory2024}. Real-time data reconstruction for this experiment is being pioneered by Teresa Montaruli's group at the University of Geneva, and a collaboration would benefit both groups.


\subsection{WP2: Real-Time, Model-Independent New Physics Detection through Open-World Learning}

\subsection{Model-independent searches}
%  A limiting factor in the search for New Physics is related to how such searches are performed. Searches for new physics processes at particle colliders are usually performed as \textit{blind searches}. Such searches proceed by defining a region of interest in the parameter space, using simulated data of the signal and the Standard Model background processes in order to enhance the data purity. The data is only looked at in the very end where it is tested for the presence of signal through a simultaneous fit of the signal and background probability distributions, hoping to extract a non-zero signal component.

% Hundreds of such searches have been performed for hundreds of different potential new particles, but thus far none have been discovered. Despite this, there are still regions of the data that have not yet been probed for the presence of a signal. This has led to an increased interest in more \textit{model-agnostic} search strategies.

% ML-based anomaly detection methods have especially been gaining popularity in particle physics as a way of extracting potential new physics signals in a model-agnostic way, by rephrasing the problem as an out-of-distribution detecting task. In this context, model-agnostic refers to assuming no, or at least minimal, prior information regarding the physical model describing the new-physics phenomena. A typical search for new physics signatures involves looking for a specific signal and maximizing the analysis sensitivity for that single model. This analysis is not useful to investigate other new physics models. In an anomaly detection driven search, however, the aim is to be model-agnostic and only look for deviations from the background. This is less sensitive to any model that is biased to a specific signal, yet, it enables the simultaneous search of multiple new physics scenarios. An additional advantage of anomaly detection is that it allows algorithms to undergo direct training on unlabeled data.

\subsection{Novel class detection with Open World Learning}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/tSNE_2D_true_labels_10.png}
     \includegraphics[width=0.49\textwidth]{figures/tSNE_2D_orca_pred_labels_10.png}
    \caption{A t-SNE projection of the embedding space. The colors correspond to the true (left) labels and the predicted labels by ORCA (right). By Kyle Metzger (semester student) }
    \label{fig:orca}
\end{figure*}
Traditional machine learning operates under the \textit{closed-world assumption}, meaning the classes seen during training are expected to be the same during testing, with no new categories or objects appearing. However, a more realistic approach involves preparing for unseen classes during testing, or open-world scenarios. The objective here is to develop algorithms capable of classifying known data while also identifying and learning from unknown categories. This concept is termed \textit{open-world learning}~\cite{DBLP:journals/corr/abs-2102-03526}. In addition to detecting unseen categories, open-world learning involves the incremental or continual assimilation of these new classes. This way of learning is becoming crucial as AI agents increasingly need to interact with real-world environments. Examples include chatbots and self-driving cars, which cannot predict or limit their exposure to only previously seen scenarios. A chatbot cannot anticipate every possible user input, nor can a self-driving car assume it has been exposed to all possible real-world scenarios.

This is a similar setup to what we encounter in particle physics analysis: Our algorithms are typically trained on familiar classes, specifically Standard Model processes, for which we have created synthetic data. However, when these algorithms are applied on actual data, there's a chance that the real data may also contain potential New Physics processes. These would appear as new, unidentified classes within the data. Our goal is to identify and analyze these particular events.

Together with an ETH Z端rich semester student, I have been exploring this way of detecting New Physics as novel classes using the ORCA~\cite{DBLP:journals/corr/abs-2102-03526} paradigm. Assuming we have a range of known classes, which correspond to Standard Model processes, we train an algorithm to identify these classes based on labeled Standard Model Monte Carlo. When we apply this trained algorithm to data, it will itself attempt to assign that new data to a known class (SM) or a novel class (Beyond SM). This workflow is illustrated in Figure~\ref{fig:ow}.
\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/ow_np.pdf}
    \caption{An illustration of the open-world learning for new physics discover workflow. Starting from a labeled dataset and a powerful data representation, an algorithm is trained to identify novel classes in an unseen dataset}
    \label{fig:ow}
\end{figure*}

An initial result is shown in Figure~\ref{fig:orca}. Training an algorithm on known SM processes only, we attempt to discover novel classes in a never seen before dataset. This dataset mimics the SM processes we would expect to see in CMS, but also has a few novel signals injected that the algorithm has not seen during training. We observe that we are able to accurately identify these novel classes, without having been given explicit information about them.

The initial classifier needs to provide a powerful embedding of particle physics data. For this we will again neet to explore \textit{foundation models}, e.g. like the one introduced in Ref.~\cite{foundation}.


\subsection{Project components}

In this project we will design an open world novel class detection algorithm that will run in semi real-time in the 40 MHz scouting system. The most important ingredients are:
\begin{enumerate}
    \item The initial classifier needs to be extremely powerful. We will design a backbone/foundation ML model based on SM simulation that incorporates a powerful embedding allowing it to discriminate between various known classes, but also be powerful enough to identify a wide variety of unknown (potential) classes (using techniques like contrastive learning).
    \item We must design the algorithm to run in semi real-time in the scouting online processing and provide summary statistics that are powerful yet contain enough information to let us further understand the properties of potential novel classes that are identified. Storing all of the scouting data would correspond to O(100) Pb/year, so the data must be reduced to something manageable. 
    \item We must design a rigorous interpretability framework. If novel classes are discovered, we need to understand what sets them aside from seen classes using powerful clustering techniques like self-clustering maps~\cite{6790553}.
\end{enumerate}

Running a real-time, model-independent search in the scouting system using modern open world AI is a fundamentally new way to search for New Physics in CMS without theory assumptions or event filtering bias. It would open the door to a wide range of hitherto uncovered New Physics phase-space.

\section{Summary}
\begin{figure*}[bht!]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/GanntChart_SG.pdf}
    \caption{A detailed project schedule for the two work packages. The orange line corresponds to the PI and postdoc and the purple to the PhD student.}
    \label{fig:gannt}
\end{figure*}
An overview and timeline over both projects proposed here is shown in Figure~\ref{fig:gannt}.
Both are fundamentally new ways of analyzing and reconstructing data in the Level-1 hardware trigger. As we move towards HL-LHC, both will be crucial in order to make sure we maximize our sensitivity to potential New Physics. They require knowledge across a wide range of topics like Machine Learning, FPGA programming, New Physics searches and efficient inference on specialized hardware. I believe I have the necessary experience within all of these. Besides the projects proposed above, I also wish to continue building our group as a center for fast inference in particle physics in Europe and collaborate with researchers from other scientific domains and industry. As coordinator of the Targeted Systems group at the Accelerated AI Algorithms for Data-Driven Discovery (A3D3) Institute, as well as a coordinator of the Fast Machine Learning for Science Lab, I am one of the key contributors to the field of real-time AI in scientific applications (see CV). Within ETH, I see a great opportunity to build bonds with other departments. I have collaborated with T. Delbrueck the Institute of Neuroinformatics, and have initiated collaborations with the group of L. Benini at the ETH Future Computing Laboratory and G. Alonso at the ETH Computer Systems group. These are projects I wish to continue in the future, and their help will be important in realizing a distributed inference algorithm in the Level-1 trigger. Additionally, I have an ongoing collaboration with Google Zurich on designing low-latency algorithms on FPGAs (see CV), which will continue to be crucial for the design of ML-based algorithms for the CMS Level-1 trigger during HL-LHC. As a member of the core team assisting and training the CMS Collaboration as a whole to perform nanosecond inference in the Level-1 trigger, I believe our group at ETH will play a crucial role for the success of CMS during HL-LHC.

\bibliographystyle{cms_unsrt}
\bibliography{bibliography}

\end{document}

